{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b8c4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Headers\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n",
      "                          Headers\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the header tags on the page\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "# Create an empty list to store the header text\n",
    "header_text = []\n",
    "\n",
    "# Loop through each header tag and add the text to the list\n",
    "for header in headers:\n",
    "    header_text.append(header.text.strip())\n",
    "\n",
    "# Create a pandas DataFrame from the header text list\n",
    "df = pd.DataFrame({'Headers': header_text})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the header tags on the page\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "# Create an empty list to store the header text\n",
    "header_text = []\n",
    "\n",
    "# Loop through each header tag and add the text to the list\n",
    "for header in headers:\n",
    "    header_text.append(header.text.strip())\n",
    "\n",
    "# Create a pandas DataFrame from the header text list\n",
    "df = pd.DataFrame({'Headers': header_text})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839924e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://www.imdb.com/chart/top/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the movie titles, ratings, and release years on the page\n",
    "titles = []\n",
    "ratings = []\n",
    "years = []\n",
    "for movie in soup.select('td.titleColumn'):\n",
    "    title = movie.select('a')[0].get_text()\n",
    "    titles.append(title)\n",
    "    rating = float(movie.select('strong')[0].get_text())\n",
    "    ratings.append(rating)\n",
    "    year = int(movie.select('span')[0].get_text()[1:-1])\n",
    "    years.append(year)\n",
    "\n",
    "# Create a pandas DataFrame from the movie data\n",
    "df = pd.DataFrame({'Title': titles, 'Rating': ratings, 'Year': years})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the movie titles, ratings, and release years on the page\n",
    "titles = []\n",
    "ratings = []\n",
    "years = []\n",
    "for movie in soup.select('td.titleColumn'):\n",
    "    title = movie.select('a')[0].get_text()\n",
    "    titles.append(title)\n",
    "    rating = float(movie.select('strong')[0].get_text())\n",
    "    ratings.append(rating)\n",
    "    year = int(movie.select('span')[0].get_text()[1:-1])\n",
    "    years.append(year)\n",
    "\n",
    "# Create a pandas DataFrame from the movie data\n",
    "df = pd.DataFrame({'Title': titles, 'Rating': ratings, 'Year': years})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c798a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the former presidents and their terms of office on the page\n",
    "presidents = []\n",
    "terms = []\n",
    "for row in soup.select('div.col-md-12 table tbody tr'):\n",
    "    columns = row.select('td')\n",
    "    name = columns[0].get_text()\n",
    "    term = columns[1].get_text()\n",
    "    presidents.append(name)\n",
    "    terms.append(term)\n",
    "\n",
    "# Create a pandas DataFrame from the former presidents data\n",
    "df = pd.DataFrame({'Name': presidents, 'Term of Office': terms})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URLs to scrape\n",
    "team_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "batsman_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "bowler_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "\n",
    "# Send a GET request to each URL\n",
    "team_response = requests.get(team_url)\n",
    "batsman_response = requests.get(batsman_url)\n",
    "bowler_response = requests.get(bowler_url)\n",
    "\n",
    "# Parse the HTML content of each page with BeautifulSoup\n",
    "team_soup = BeautifulSoup(team_response.content, 'html.parser')\n",
    "batsman_soup = BeautifulSoup(batsman_response.content, 'html.parser')\n",
    "bowler_soup = BeautifulSoup(bowler_response.content, 'html.parser')\n",
    "\n",
    "# Extract the data for the top 10 ODI teams\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "for row in team_soup.select('table.table tbody tr')[:10]:\n",
    "    columns = row.select('td')\n",
    "    team = columns[1].select('span')[1].get_text().strip()\n",
    "    teams.append(team)\n",
    "    match = columns[2].get_text()\n",
    "    matches.append(match)\n",
    "    point = columns[3].get_text()\n",
    "    points.append(point)\n",
    "    rating = columns[4].get_text()\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a pandas DataFrame for the top 10 ODI teams\n",
    "team_df = pd.DataFrame({'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "\n",
    "# Extract the data for the top 10 ODI batsmen\n",
    "batsmen = []\n",
    "teams = []\n",
    "ratings = []\n",
    "for row in batsman_soup.select('table.table tbody tr')[:10]:\n",
    "    columns = row.select('td')\n",
    "    batsman = columns[1].select('a')[0].get_text().strip()\n",
    "    batsmen.append(batsman)\n",
    "    team = columns[1].select('span')[1].get_text().strip()\n",
    "    teams.append(team)\n",
    "    rating = columns[3].get_text()\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a pandas DataFrame for the top 10 ODI batsmen\n",
    "batsman_df = pd.DataFrame({'Batsman': batsmen, 'Team': teams, 'Rating': ratings})\n",
    "\n",
    "# Extract the data for the top 10 ODI bowlers\n",
    "bowlers = []\n",
    "teams = []\n",
    "ratings = []\n",
    "for row in bowler_soup.select('table.table tbody tr')[:10]:\n",
    "    columns = row.select('td')\n",
    "    bowler = columns[1].select('a')[0].get_text().strip()\n",
    "    bowlers.append(bowler)\n",
    "    team = columns[1].select('span')[1].get_text().strip()\n",
    "    teams.append(team)\n",
    "    rating = columns[3].get_text()\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a pandas DataFrame for the top 10 ODI bowlers\n",
    "bowler_df = pd.DataFrame({'Bowler': bowlers, 'Team': teams, 'Rating': ratings})\n",
    "\n",
    "# Print the DataFrames\n",
    "print('Top 10 ODI Teams:')\n",
    "print(team_df)\n",
    "print()\n",
    "print('Top 10 ODI Batsmen:')\n",
    "print(batsman_df)\n",
    "print()\n",
    "print('Top 10 ODI Bowlers:')\n",
    "print(bowler_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URLs to scrape\n",
    "team_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "batswoman_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "allrounder_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "\n",
    "# Send a GET request to each URL\n",
    "team_response = requests.get(team_url)\n",
    "batswoman_response = requests.get(batswoman_url)\n",
    "allrounder_response = requests.get(allrounder_url)\n",
    "\n",
    "# Parse the HTML content of each page with BeautifulSoup\n",
    "team_soup = BeautifulSoup(team_response.content, 'html.parser')\n",
    "batswoman_soup = BeautifulSoup(batswoman_response.content, 'html.parser')\n",
    "allrounder_soup = BeautifulSoup(allrounder_response.content, 'html.parser')\n",
    "\n",
    "# Extract the data for the top 10 ODI teams\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "for row in team_soup.select('table.table tbody tr')[:10]:\n",
    "    columns = row.select('td')\n",
    "    team = columns[1].select('span')[1].get_text().strip()\n",
    "    teams.append(team)\n",
    "    match = columns[2].get_text()\n",
    "    matches.append(match)\n",
    "    point = columns[3].get_text()\n",
    "    points.append(point)\n",
    "    rating = columns[4].get_text()\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a pandas DataFrame for the top 10 ODI teams\n",
    "team_df = pd.DataFrame({'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "\n",
    "# Extract the data for the top 10 ODI batswomen\n",
    "batswomen = []\n",
    "teams = []\n",
    "ratings = []\n",
    "for row in batswoman_soup.select('table.table tbody tr')[:10]:\n",
    "    columns = row.select('td')\n",
    "    batswoman = columns[1].select('a')[0].get_text().strip()\n",
    "    batswomen.append(batswoman)\n",
    "    team = columns[1].select('span')[1].get_text().strip()\n",
    "    teams.append(team)\n",
    "    rating = columns[3].get_text()\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a pandas DataFrame for the top 10 ODI batswomen\n",
    "batswoman_df = pd.DataFrame({'Batswoman': batswomen, 'Team': teams, 'Rating': ratings})\n",
    "\n",
    "# Extract the data for the top 10 ODI all-rounders\n",
    "allrounders = []\n",
    "teams = []\n",
    "ratings = []\n",
    "for row in allrounder_soup.select('table.table tbody tr')[:10]:\n",
    "    columns = row.select('td')\n",
    "    allrounder = columns[1].select('a')[0].get_text().strip()\n",
    "    allrounders.append(allrounder)\n",
    "    team = columns[1].select('span')[1].get_text().strip()\n",
    "    teams.append(team)\n",
    "    rating = columns[3].get_text()\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a pandas DataFrame for the top 10 ODI all-rounders\n",
    "allrounder_df = pd.DataFrame({'All-Rounder': allrounders, 'Team': teams, 'Rating': ratings})\n",
    "\n",
    "# Print the DataFrames\n",
    "print('Top 10 ODI Women’s Teams:')\n",
    "print(team_df)\n",
    "print()\n",
    "print('Top 10 ODI Women’s Batswomen:')\n",
    "print(batswoman_df)\n",
    "print()\n",
    "print('Top 10 ODI Women’s Bowlers:')\n",
    "print(bowler_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb587e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the data for each news article\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "for article in soup.select('div.Card-textContainer'):\n",
    "    headline = article.select('a.Card-title')[0].get_text().strip()\n",
    "    headlines.append(headline)\n",
    "    time = article.select('time')[0]['datetime']\n",
    "    times.append(time)\n",
    "    link = article.select('a.Card-title')[0]['href']\n",
    "    links.append(link)\n",
    "\n",
    "# Create a pandas DataFrame for the news data\n",
    "df = pd.DataFrame({'Headline': headlines, 'Time': times, 'NewsLink': links})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6754bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the data for each article\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "for article in soup.select('div.most-downloaded-articles-item'):\n",
    "    title = article.select('a.most-downloaded-articles-link')[0].get_text().strip()\n",
    "    titles.append(title)\n",
    "    author = article.select('div.js-multiple-authors')[0].get_text().strip()\n",
    "    authors.append(author)\n",
    "    date = article.select('span.js-article-date')[0].get_text().strip()\n",
    "    dates.append(date)\n",
    "    url = article.select('a.most-downloaded-articles-link')[0]['href']\n",
    "    urls.append(url)\n",
    "\n",
    "# Create a pandas DataFrame for the article data\n",
    "df = pd.DataFrame({'Paper Title': titles, 'Authors': authors, 'Published Date': dates, 'Paper URL': urls})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da85562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the restaurant names\n",
    "restaurants = []\n",
    "for restaurant in soup.select('h2.restnt-name'):\n",
    "    name = restaurant.get_text().strip()\n",
    "    restaurants.append(name)\n",
    "\n",
    "# Create a pandas DataFrame for the restaurant data\n",
    "df = pd.DataFrame({'Restaurant Name': restaurants})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
