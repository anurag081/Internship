{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Wikipedia URL\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table that contains the most viewed videos\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "\n",
    "# Initialize lists to store the details\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Iterate over each row in the table (skipping the header row)\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    \n",
    "    # Extract the details from each column\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip().replace(\",\", \"\")  # Remove comma from views count\n",
    "    \n",
    "    # Add the details to the respective lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}\")\n",
    "    print(f\"Name: {name_list[i]}\")\n",
    "    print(f\"Artist: {artist_list[i]}\")\n",
    "    print(f\"Upload Date: {upload_date_list[i]}\")\n",
    "    print(f\"Views: {views_list[i]}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the BCCI.tv home page\n",
    "url = \"https://www.bcci.tv/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = soup.find(\"a\", text=\"International Fixtures\")[\"href\"]\n",
    "\n",
    "# Construct the URL for the international fixtures page\n",
    "fixtures_url = url + fixtures_link\n",
    "\n",
    "# Send a GET request to the international fixtures page\n",
    "response = requests.get(fixtures_url)\n",
    "\n",
    "# Create a new BeautifulSoup object for the fixtures page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container that holds the fixture details\n",
    "container = soup.find(\"div\", {\"class\": \"fixture-list\"})\n",
    "\n",
    "# Find all the fixtures\n",
    "fixtures = container.find_all(\"div\", {\"class\": \"fixture-widget\"})\n",
    "\n",
    "# Initialize lists to store the details\n",
    "match_title_list = []\n",
    "series_list = []\n",
    "place_list = []\n",
    "date_list = []\n",
    "time_list = []\n",
    "\n",
    "# Iterate over each fixture\n",
    "for fixture in fixtures:\n",
    "    # Extract the details\n",
    "    match_title = fixture.find(\"span\", {\"class\": \"fixture-widget__description\"}).text.strip()\n",
    "    series = fixture.find(\"span\", {\"class\": \"u-unskewed-text\"}).text.strip()\n",
    "    place = fixture.find(\"p\", {\"class\": \"fixture-widget__additional-info\"}).text.strip()\n",
    "    date = fixture.find(\"span\", {\"class\": \"fixture-widget__date\"}).text.strip()\n",
    "    time = fixture.find(\"span\", {\"class\": \"fixture-widget__time\"}).text.strip()\n",
    "    \n",
    "    # Add the details to the respective lists\n",
    "    match_title_list.append(match_title)\n",
    "    series_list.append(series)\n",
    "    place_list.append(place)\n",
    "    date_list.append(date)\n",
    "    time_list.append(time)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(match_title_list)):\n",
    "    print(f\"Match Title: {match_title_list[i]}\")\n",
    "    print(f\"Series: {series_list[i]}\")\n",
    "    print(f\"Place: {place_list[i]}\")\n",
    "    print(f\"Date: {date_list[i]}\")\n",
    "    print(f\"Time: {time_list[i]}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edacea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the StatisticsTimes home page\n",
    "url = \"http://statisticstimes.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the Economy page\n",
    "economy_link = soup.find(\"a\", text=\"Economy\")[\"href\"]\n",
    "\n",
    "# Construct the URL for the Economy page\n",
    "economy_url = url + economy_link\n",
    "\n",
    "# Send a GET request to the Economy page\n",
    "response = requests.get(economy_url)\n",
    "\n",
    "# Create a new BeautifulSoup object for the Economy page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the State-wise GDP page\n",
    "gdp_link = soup.find(\"a\", text=\"GDP of Indian states\")[\"href\"]\n",
    "\n",
    "# Construct the URL for the State-wise GDP page\n",
    "gdp_url = url + gdp_link\n",
    "\n",
    "# Send a GET request to the State-wise GDP page\n",
    "response = requests.get(gdp_url)\n",
    "\n",
    "# Create a new BeautifulSoup object for the State-wise GDP page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table that contains the state-wise GDP details\n",
    "table = soup.find(\"table\", {\"class\": \"display\"})\n",
    "\n",
    "# Initialize lists to store the details\n",
    "rank_list = []\n",
    "state_list = []\n",
    "gsdp_18_19_list = []\n",
    "gsdp_19_20_list = []\n",
    "share_18_19_list = []\n",
    "gdp_billion_list = []\n",
    "\n",
    "# Iterate over each row in the table (skipping the header row)\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    \n",
    "    # Extract the details from each column\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gsdp_18_19 = columns[2].text.strip()\n",
    "    gsdp_19_20 = columns[3].text.strip()\n",
    "    share_18_19 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "    \n",
    "    # Add the details to the respective lists\n",
    "    rank_list.append(rank)\n",
    "    state_list.append(state)\n",
    "    gsdp_18_19_list.append(gsdp_18_19)\n",
    "    gsdp_19_20_list.append(gsdp_19_20)\n",
    "    share_18_19_list.append(share_18_19)\n",
    "    gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}\")\n",
    "    print(f\"State: {state_list[i]}\")\n",
    "    print(f\"GSDP (18-19) - at current prices: {gsdp_18_19_list[i]}\")\n",
    "    print(f\"GSDP (19-20) - at current prices: {gsdp_19_20_list[i]}\")\n",
    "    print(f\"Share (18-19): {share_18_19_list[i]}\")\n",
    "    print(f\"GDP ($ billion): {gdp_billion_list[i]}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568de393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Send a GET request to the GitHub API for trending repositories\n",
    "url = \"https://api.github.com/search/repositories?q=stars:%3E1&sort=stars&order=desc\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Get the JSON response from the API\n",
    "data = response.json()\n",
    "\n",
    "# Extract the list of repositories from the JSON response\n",
    "repositories = data[\"items\"]\n",
    "\n",
    "# Iterate over each repository\n",
    "for repo in repositories:\n",
    "    # Extract the details\n",
    "    repo_title = repo[\"name\"]\n",
    "    repo_description = repo[\"description\"]\n",
    "    contributors_count = repo[\"contributors_url\"]\n",
    "    language_used = repo[\"language\"]\n",
    "    \n",
    "    # Get the contributors count\n",
    "    contributors_response = requests.get(contributors_count)\n",
    "    contributors_data = contributors_response.json()\n",
    "    contributors = len(contributors_data)\n",
    "    \n",
    "    # Print the details\n",
    "    print(f\"Repository Title: {repo_title}\")\n",
    "    print(f\"Repository Description: {repo_description}\")\n",
    "    print(f\"Contributors Count: {contributors}\")\n",
    "    print(f\"Language Used: {language_used}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bac758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Billboard.com home page\n",
    "url = \"https://www.billboard.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the Charts page\n",
    "charts_link = soup.find(\"a\", text=\"Charts\")[\"href\"]\n",
    "\n",
    "# Construct the URL for the Charts page\n",
    "charts_url = url + charts_link\n",
    "\n",
    "# Send a GET request to the Charts page\n",
    "response = requests.get(charts_url)\n",
    "\n",
    "# Create a new BeautifulSoup object for the Charts page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the Hot 100 page\n",
    "hot_100_link = soup.find(\"a\", text=\"Hot 100\")[\"href\"]\n",
    "\n",
    "# Construct the URL for the Hot 100 page\n",
    "hot_100_url = url + hot_100_link\n",
    "\n",
    "# Send a GET request to the Hot 100 page\n",
    "response = requests.get(hot_100_url)\n",
    "\n",
    "# Create a new BeautifulSoup object for the Hot 100 page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container that holds the song details\n",
    "container = soup.find(\"ol\", {\"class\": \"chart-list__elements\"})\n",
    "\n",
    "# Find all the song entries\n",
    "songs = container.find_all(\"li\")\n",
    "\n",
    "# Initialize lists to store the details\n",
    "song_name_list = []\n",
    "artist_name_list = []\n",
    "last_week_rank_list = []\n",
    "peak_rank_list = []\n",
    "weeks_on_board_list = []\n",
    "\n",
    "# Iterate over each song entry\n",
    "for song in songs:\n",
    "    # Extract the details\n",
    "    song_name = song.find(\"span\", {\"class\": \"chart-element__information__song\"}).text.strip()\n",
    "    artist_name = song.find(\"span\", {\"class\": \"chart-element__information__artist\"}).text.strip()\n",
    "    last_week_rank = song.find(\"span\", {\"class\": \"chart-element__meta text--last\"}).text.strip()\n",
    "    peak_rank = song.find(\"span\", {\"class\": \"chart-element__meta text--peak\"}).text.strip()\n",
    "    weeks_on_board = song.find(\"span\", {\"class\": \"chart-element__meta text--week\"}).text.strip()\n",
    "    \n",
    "    # Add the details to the respective lists\n",
    "    song_name_list.append(song_name)\n",
    "    artist_name_list.append(artist_name)\n",
    "    last_week_rank_list.append(last_week_rank)\n",
    "    peak_rank_list.append(peak_rank)\n",
    "    weeks_on_board_list.append(weeks_on_board)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(song_name_list)):\n",
    "    print(f\"Song Name: {song_name_list[i]}\")\n",
    "    print(f\"Artist Name: {artist_name_list[i]}\")\n",
    "    print(f\"Last Week Rank: {last_week_rank_list[i]}\")\n",
    "    print(f\"Peak Rank: {peak_rank_list[i]}\")\n",
    "    print(f\"Weeks on Board: {weeks_on_board_list[i]}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9873e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table that contains the book details\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Find all the rows in the table (skipping the header row)\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "# Initialize lists to store the details\n",
    "book_name_list = []\n",
    "author_name_list = []\n",
    "volumes_sold_list = []\n",
    "publisher_list = []\n",
    "genre_list = []\n",
    "\n",
    "# Iterate over each row\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "\n",
    "    # Extract the details from each column\n",
    "    book_name = columns[1].text.strip()\n",
    "    author_name = columns[2].text.strip()\n",
    "    volumes_sold = columns[3].text.strip()\n",
    "    publisher = columns[4].text.strip()\n",
    "    genre = columns[5].text.strip()\n",
    "\n",
    "    # Add the details to the respective lists\n",
    "    book_name_list.append(book_name)\n",
    "    author_name_list.append(author_name)\n",
    "    volumes_sold_list.append(volumes_sold)\n",
    "    publisher_list.append(publisher)\n",
    "    genre_list.append(genre)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(book_name_list)):\n",
    "    print(f\"Book Name: {book_name_list[i]}\")\n",
    "    print(f\"Author Name: {author_name_list[i]}\")\n",
    "    print(f\"Volumes Sold: {volumes_sold_list[i]}\")\n",
    "    print(f\"Publisher: {publisher_list[i]}\")\n",
    "    print(f\"Genre: {genre_list[i]}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4513a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the list that contains the TV series details\n",
    "tv_series_list = soup.find(\"div\", {\"class\": \"lister-list\"})\n",
    "\n",
    "# Find all the TV series entries\n",
    "tv_series_entries = tv_series_list.find_all(\"div\", {\"class\": \"lister-item mode-detail\"})\n",
    "\n",
    "# Initialize lists to store the details\n",
    "name_list = []\n",
    "year_span_list = []\n",
    "genre_list = []\n",
    "run_time_list = []\n",
    "ratings_list = []\n",
    "votes_list = []\n",
    "\n",
    "# Iterate over each TV series entry\n",
    "for tv_series_entry in tv_series_entries:\n",
    "    # Extract the details\n",
    "    name = tv_series_entry.h3.a.text.strip()\n",
    "    year_span = tv_series_entry.find(\"span\", {\"class\": \"lister-item-year text-muted unbold\"}).text.strip(\"()\")\n",
    "    genre = tv_series_entry.find(\"span\", {\"class\": \"genre\"}).text.strip()\n",
    "    run_time = tv_series_entry.find(\"span\", {\"class\": \"runtime\"}).text.strip()\n",
    "    ratings = float(tv_series_entry.find(\"span\", {\"class\": \"ipl-rating-star__rating\"}).text.strip())\n",
    "    votes = int(tv_series_entry.find(\"span\", {\"name\": \"nv\"}).text.replace(\",\", \"\"))\n",
    "    \n",
    "    # Add the details to the respective lists\n",
    "    name_list.append(name)\n",
    "    year_span_list.append(year_span)\n",
    "    genre_list.append(genre)\n",
    "    run_time_list.append(run_time)\n",
    "    ratings_list.append(ratings)\n",
    "    votes_list.append(votes)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(name_list)):\n",
    "    print(f\"Name: {name_list[i]}\")\n",
    "    print(f\"Year Span: {year_span_list[i]}\")\n",
    "    print(f\"Genre: {genre_list[i]}\")\n",
    "    print(f\"Run Time: {run_time_list[i]}\")\n",
    "    print(f\"Ratings: {ratings_list[i]}\")\n",
    "    print(f\"Votes: {votes_list[i]}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1186260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the UCI Machine Learning Repository home page\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the Show All Dataset page\n",
    "show_all_link = soup.find(\"a\", href=\"ml/datasets.php\")[\"href\"]\n",
    "\n",
    "# Construct the URL for the Show All Dataset page\n",
    "show_all_url = url + show_all_link\n",
    "\n",
    "# Send a GET request to the Show All Dataset page\n",
    "response = requests.get(show_all_url)\n",
    "\n",
    "# Create a new BeautifulSoup object for the Show All Dataset page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table that contains the dataset details\n",
    "table = soup.find(\"table\", {\"border\": \"1\"})\n",
    "\n",
    "# Find all the rows in the table (skipping the header row)\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "# Initialize lists to store the details\n",
    "dataset_name_list = []\n",
    "data_type_list = []\n",
    "task_list = []\n",
    "attribute_type_list = []\n",
    "no_of_instances_list = []\n",
    "no_of_attribute_list = []\n",
    "year_list = []\n",
    "\n",
    "# Iterate over each row\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "\n",
    "    # Extract the details from each column\n",
    "    dataset_name = columns[0].text.strip()\n",
    "    data_type = columns[1].text.strip()\n",
    "    task = columns[2].text.strip()\n",
    "    attribute_type = columns[3].text.strip()\n",
    "    no_of_instances = columns[4].text.strip()\n",
    "    no_of_attribute = columns[5].text.strip()\n",
    "    year = columns[6].text.strip()\n",
    "\n",
    "    # Add the details to the respective lists\n",
    "    dataset_name_list.append(dataset_name)\n",
    "    data_type_list.append(data_type)\n",
    "    task_list.append(task)\n",
    "    attribute_type_list.append(attribute_type)\n",
    "    no_of_instances_list.append(no_of_instances)\n",
    "    no_of_attribute_list.append(no_of_attribute)\n",
    "    year_list.append(year)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(dataset_name_list)):\n",
    "    print(f\"Dataset Name: {dataset_name_list[i]}\")\n",
    "    print(f\"Data Type: {data_type_list[i]}\")\n",
    "    print(f\"Task: {task_list[i]}\")\n",
    "    print(f\"Attribute Type: {attribute_type_list[i]}\")\n",
    "    print(f\"No of Instances: {no_of_instances_list[i]}\")\n",
    "    print(f\"No of Attributes: {no_of_attribute_list[i]}\")\n",
    "    print(f\"Year: {year_list[i]}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Naukri.com recruiters page\n",
    "url = \"https://www.naukri.com/hr-recruiters-consultants\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the search input field and form on the page\n",
    "search_form = soup.find(\"form\", {\"class\": \"srchSec\"})\n",
    "search_input = search_form.find(\"input\", {\"id\": \"root-autocomplete\"})\n",
    "search_button = search_form.find(\"button\")\n",
    "\n",
    "# Set the search value to \"Data Science\" and submit the form\n",
    "search_input[\"value\"] = \"Data Science\"\n",
    "response = requests.post(url, data=search_form)\n",
    "\n",
    "# Create a new BeautifulSoup object for the search results page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the list that contains the recruiter details\n",
    "recruiter_list = soup.find(\"section\", {\"id\": \"recInfo\"})\n",
    "\n",
    "# Find all the recruiter entries\n",
    "recruiter_entries = recruiter_list.find_all(\"div\", {\"class\": \"recInfo\"})\n",
    "\n",
    "# Initialize lists to store the details\n",
    "name_list = []\n",
    "designation_list = []\n",
    "company_list = []\n",
    "skills_list = []\n",
    "location_list = []\n",
    "\n",
    "# Iterate over each recruiter entry\n",
    "for recruiter_entry in recruiter_entries:\n",
    "    # Extract the details\n",
    "    name = recruiter_entry.find(\"span\", {\"class\": \"fl ellipsis\"}).text.strip()\n",
    "    designation = recruiter_entry.find(\"span\", {\"class\": \"designation\"}).text.strip()\n",
    "    company = recruiter_entry.find(\"span\", {\"class\": \"ellipsis\"}).text.strip()\n",
    "    skills = recruiter_entry.find(\"div\", {\"class\": \"rec-s skil\"}).text.strip()\n",
    "    location = recruiter_entry.find(\"div\", {\"class\": \"recLoc\"}).text.strip()\n",
    "    \n",
    "    # Add the details to the respective lists\n",
    "    name_list.append(name)\n",
    "    designation_list.append(designation)\n",
    "    company_list.append(company)\n",
    "    skills_list.append(skills)\n",
    "    location_list.append(location)\n",
    "\n",
    "# Print the scraped details\n",
    "for i in range(len(name_list)):\n",
    "    print(f\"Name: {name_list[i]}\")\n",
    "    print(f\"Designation: {designation_list[i]}\")\n",
    "    print(f\"Company: {company_list[i]}\")\n",
    "    print(f\"Skills They Hire for: {skills_list[i]}\")\n",
    "    print(f\"Location: {location_list[i]}\")\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
