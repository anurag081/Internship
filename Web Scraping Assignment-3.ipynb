{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(product):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    params = {\n",
    "        \"k\": product\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    product_elements = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "    \n",
    "    for element in product_elements:\n",
    "        title_element = element.find(\"span\", {\"class\": \"a-size-medium\"})\n",
    "        price_element = element.find(\"span\", {\"class\": \"a-offscreen\"})\n",
    "        \n",
    "        if title_element and price_element:\n",
    "            title = title_element.text.strip()\n",
    "            price = price_element.text.strip()\n",
    "            \n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Price: {price}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "# Get the product to search from the user\n",
    "product = input(\"Enter a product to search on Amazon: \")\n",
    "\n",
    "# Perform the search\n",
    "search_amazon_products(product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51de389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    params = {\n",
    "        \"k\": product\n",
    "    }\n",
    "\n",
    "    product_data = []\n",
    "    page_limit = 3\n",
    "    page_count = 0\n",
    "    \n",
    "    while page_count < page_limit:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        product_elements = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "        \n",
    "        if not product_elements:  # No more pages for the product\n",
    "            break\n",
    "\n",
    "        for element in product_elements:\n",
    "            title_element = element.find(\"span\", {\"class\": \"a-size-medium\"})\n",
    "            price_element = element.find(\"span\", {\"class\": \"a-offscreen\"})\n",
    "            return_element = element.find(\"span\", {\"class\": \"a-truncate-cut\"})\n",
    "            delivery_element = element.find(\"span\", {\"class\": \"s-info-strip-text\"})\n",
    "            availability_element = element.find(\"span\", {\"class\": \"a-size-base\"})\n",
    "            url_element = element.find(\"a\", {\"class\": \"a-link-normal\"})\n",
    "\n",
    "            title = title_element.text.strip() if title_element else \"-\"\n",
    "            price = price_element.text.strip() if price_element else \"-\"\n",
    "            return_policy = return_element.text.strip() if return_element else \"-\"\n",
    "            delivery = delivery_element.text.strip() if delivery_element else \"-\"\n",
    "            availability = availability_element.text.strip() if availability_element else \"-\"\n",
    "            url = \"https://www.amazon.in\" + url_element[\"href\"] if url_element else \"-\"\n",
    "\n",
    "            product_data.append({\n",
    "                \"Brand Name\": \"-\",\n",
    "                \"Name of the Product\": title,\n",
    "                \"Price\": price,\n",
    "                \"Return/Exchange\": return_policy,\n",
    "                \"Expected Delivery\": delivery,\n",
    "                \"Availability\": availability,\n",
    "                \"Product URL\": url\n",
    "            })\n",
    "\n",
    "        # Go to the next page\n",
    "        next_page_element = soup.find(\"a\", {\"class\": \"s-pagination-item s-pagination-next\"})\n",
    "        if next_page_element:\n",
    "            params[\"page\"] = next_page_element[\"href\"]\n",
    "            page_count += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return product_data\n",
    "\n",
    "# Get the product to search from the user\n",
    "product = input(\"Enter a product to search on Amazon: \")\n",
    "\n",
    "# Scrape the product details\n",
    "products = scrape_product_details(product)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"amazon_products.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    # Configure Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run Chrome in headless mode (without opening GUI)\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # Set path to your chromedriver executable\n",
    "    chromedriver_path = \"path_to_chromedriver\"\n",
    "\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome(chromedriver_path, options=options)\n",
    "\n",
    "    # Open Google Images\n",
    "    driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "    # Find the search bar element\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "\n",
    "    # Enter the keyword and press Enter\n",
    "    search_bar.send_keys(keyword + Keys.RETURN)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Scroll down to load more images\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Find all image elements on the page\n",
    "    image_elements = driver.find_elements_by_css_selector(\".rg_i\")\n",
    "\n",
    "    # Create a directory to save the images\n",
    "    save_dir = keyword.lower()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Download the images\n",
    "    for i, image_element in enumerate(image_elements[:num_images]):\n",
    "        image_url = image_element.get_attribute(\"src\")\n",
    "        if image_url:\n",
    "            response = requests.get(image_url)\n",
    "            with open(os.path.join(save_dir, f\"{keyword}_{i+1}.jpg\"), \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Keywords and number of images to scrape\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images = 10\n",
    "\n",
    "# Scrape images for each keyword\n",
    "for keyword in keywords:\n",
    "    scrape_images(keyword, num_images)\n",
    "    print(f\"Scraped {num_images} images for '{keyword}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = base_url + \"/search\"\n",
    "    params = {\n",
    "        \"q\": search_query\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, params=params)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    product_elements = soup.find_all(\"div\", {\"class\": \"_1AtVbE\"})\n",
    "    \n",
    "    product_data = []\n",
    "    for element in product_elements:\n",
    "        brand_element = element.find(\"div\", {\"class\": \"_4rR01T\"})\n",
    "        name_element = element.find(\"a\", {\"class\": \"IRpwTa\"})\n",
    "        url_element = element.find(\"a\", {\"class\": \"_1fQZEK\"})\n",
    "        price_element = element.find(\"div\", {\"class\": \"_30jeq3 _1_WHN1\"})\n",
    "        colour_element = element.find(\"div\", {\"class\": \"_4rR01T\"})\n",
    "        ram_element = element.find(\"ul\", {\"class\": \"_1xgFaf\"})\n",
    "        storage_element = element.find(\"ul\", {\"class\": \"_1xgFaf\"})\n",
    "        primary_camera_element = element.find(\"ul\", {\"class\": \"_1xgFaf\"})\n",
    "        secondary_camera_element = element.find(\"ul\", {\"class\": \"_1xgFaf\"})\n",
    "        display_element = element.find(\"ul\", {\"class\": \"_1xgFaf\"})\n",
    "        battery_element = element.find(\"ul\", {\"class\": \"_1xgFaf\"})\n",
    "\n",
    "        brand = brand_element.text.strip() if brand_element else \"-\"\n",
    "        name = name_element.text.strip() if name_element else \"-\"\n",
    "        url = base_url + url_element[\"href\"] if url_element else \"-\"\n",
    "        price = price_element.text.strip().replace(\"â‚¹\", \"\") if price_element else \"-\"\n",
    "        colour = colour_element.contents[2].strip() if colour_element else \"-\"\n",
    "        ram = ram_element.contents[4].text.strip() if ram_element else \"-\"\n",
    "        storage = storage_element.contents[8].text.strip() if storage_element else \"-\"\n",
    "        primary_camera = primary_camera_element.contents[12].text.strip() if primary_camera_element else \"-\"\n",
    "        secondary_camera = secondary_camera_element.contents[16].text.strip() if secondary_camera_element else \"-\"\n",
    "        display = display_element.contents[20].text.strip() if display_element else \"-\"\n",
    "        battery = battery_element.contents[24].text.strip() if battery_element else \"-\"\n",
    "\n",
    "        product_data.append({\n",
    "            \"Brand Name\": brand,\n",
    "            \"Smartphone Name\": name,\n",
    "            \"Colour\": colour,\n",
    "            \"RAM\": ram,\n",
    "            \"Storage(ROM)\": storage,\n",
    "            \"Primary Camera\": primary_camera,\n",
    "            \"Secondary Camera\": secondary_camera,\n",
    "            \"Display Size\": display,\n",
    "            \"Battery Capacity\": battery,\n",
    "            \"Price\": price,\n",
    "            \"Product URL\": url\n",
    "        })\n",
    "\n",
    "    return product_data\n",
    "\n",
    "# Get the smartphone to search from the user\n",
    "search_query = input(\"Enter a smartphone to search on Flipkart: \")\n",
    "\n",
    "# Scrape the smartphone details\n",
    "products = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"flipkart_smartphones.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b62b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_coordinates(city):\n",
    "    base_url = \"https://www.google.com/maps/search/\"\n",
    "    query = city.replace(\" \", \"+\")\n",
    "    url = base_url + query\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the div element containing the coordinates\n",
    "    coordinates_div = soup.find(\"meta\", {\"itemprop\": \"image\"})\n",
    "    if coordinates_div:\n",
    "        coordinates = coordinates_div[\"content\"].split(\"=\")[-1].split(\",\")\n",
    "        latitude = coordinates[0]\n",
    "        longitude = coordinates[1]\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Get the city to search from the user\n",
    "city = input(\"Enter a city to search on Google Maps: \")\n",
    "\n",
    "# Scrape the coordinates\n",
    "latitude, longitude = scrape_coordinates(city)\n",
    "\n",
    "# Display the coordinates\n",
    "if latitude and longitude:\n",
    "    print(f\"Latitude: {latitude}\")\n",
    "    print(f\"Longitude: {longitude}\")\n",
    "else:\n",
    "    print(\"Coordinates not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba903072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_funding_deals():\n",
    "    url = \"https://trak.in/india-startup-funding-investment-2015/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"id\": \"tablepress-48\"})\n",
    "\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    data = []\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) >= 5:\n",
    "            date = cells[1].text.strip()\n",
    "            startup = cells[2].text.strip()\n",
    "            industry = cells[3].text.strip()\n",
    "            sub_vertical = cells[4].text.strip()\n",
    "            city = cells[5].text.strip()\n",
    "            investor = cells[6].text.strip()\n",
    "            investment_type = cells[7].text.strip()\n",
    "            amount = cells[8].text.strip()\n",
    "\n",
    "            data.append({\n",
    "                \"Date\": date,\n",
    "                \"Startup\": startup,\n",
    "                \"Industry\": industry,\n",
    "                \"Sub Vertical\": sub_vertical,\n",
    "                \"City\": city,\n",
    "                \"Investor\": investor,\n",
    "                \"Investment Type\": investment_type,\n",
    "                \"Amount\": amount\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape the funding deals\n",
    "funding_deals = scrape_funding_deals()\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(funding_deals)\n",
    "\n",
    "# Filter the DataFrame for the second quarter (January to March 2021)\n",
    "start_date = \"2021-01-01\"\n",
    "end_date = \"2021-03-31\"\n",
    "filtered_df = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "filtered_df.to_csv(\"funding_deals_q2_2021.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    laptops = soup.find_all(\"div\", {\"class\": \"TopNumbeHeading\"})\n",
    "    details = soup.find_all(\"div\", {\"class\": \"detail-list\"})\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(laptops)):\n",
    "        laptop_name = laptops[i].text.strip()\n",
    "        laptop_details = details[i].find_all(\"li\")\n",
    "\n",
    "        specifications = {}\n",
    "        for detail in laptop_details:\n",
    "            key = detail.find(\"div\", {\"class\": \"prod-spec\"}).text.strip()\n",
    "            value = detail.find(\"div\", {\"class\": \"prod-specs-value\"}).text.strip()\n",
    "            specifications[key] = value\n",
    "\n",
    "        data.append({\n",
    "            \"Laptop Name\": laptop_name,\n",
    "            \"Specifications\": specifications\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape the gaming laptops details\n",
    "gaming_laptops = scrape_gaming_laptops()\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(gaming_laptops)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"gaming_laptops_details.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829869fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"class\": \"data\"})\n",
    "\n",
    "    rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")\n",
    "        rank = cells[0].text.strip()\n",
    "        name = cells[1].text.strip()\n",
    "        net_worth = cells[2].text.strip()\n",
    "        age = cells[3].text.strip()\n",
    "        citizenship = cells[4].text.strip()\n",
    "        source = cells[5].text.strip()\n",
    "        industry = cells[6].text.strip()\n",
    "\n",
    "        data.append({\n",
    "            \"Rank\": rank,\n",
    "            \"Name\": name,\n",
    "            \"Net Worth\": net_worth,\n",
    "            \"Age\": age,\n",
    "            \"Citizenship\": citizenship,\n",
    "            \"Source\": source,\n",
    "            \"Industry\": industry\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape the billionaire details\n",
    "billionaires = scrape_billionaires()\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(billionaires)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"billionaires_details.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up your YouTube Data API key\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "# Specify the video ID of the YouTube video\n",
    "VIDEO_ID = \"YOUR_VIDEO_ID\"\n",
    "\n",
    "# Specify the maximum number of comments to extract\n",
    "MAX_COMMENTS = 500\n",
    "\n",
    "def get_video_comments(video_id, api_key, max_comments):\n",
    "    # Construct the API endpoint URL\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&maxResults={max_comments}&key={api_key}\"\n",
    "\n",
    "    # Send the API request\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    comments = []\n",
    "    for item in data[\"items\"]:\n",
    "        comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "        upvotes = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "        time = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "\n",
    "        # Convert the time to a human-readable format\n",
    "        time = datetime.strptime(time, \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        comments.append({\n",
    "            \"Comment\": comment,\n",
    "            \"Upvotes\": upvotes,\n",
    "            \"Time\": time\n",
    "        })\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Get the video comments\n",
    "video_comments = get_video_comments(VIDEO_ID, API_KEY, MAX_COMMENTS)\n",
    "\n",
    "# Display the comments\n",
    "for comment in video_comments:\n",
    "    print(f\"Comment: {comment['Comment']}\")\n",
    "    print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "    print(f\"Time: {comment['Time']}\")\n",
    "    print()\n",
    "\n",
    "# Save the comments to a JSON file\n",
    "with open(\"video_comments.json\", \"w\") as file:\n",
    "    json.dump(video_comments, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6441880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels():\n",
    "    url = \"https://www.hostelworld.com/search?city=London&country=England\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    hostel_items = soup.find_all(\"div\", {\"class\": \"property-card\"})\n",
    "\n",
    "    data = []\n",
    "    for item in hostel_items:\n",
    "        hostel_name = item.find(\"h2\", {\"class\": \"title-row\"}).text.strip()\n",
    "        distance = item.find(\"span\", {\"class\": \"distance\"}).text.strip()\n",
    "        rating = item.find(\"div\", {\"class\": \"score orange\"}).text.strip()\n",
    "        total_reviews = item.find(\"div\", {\"class\": \"reviews\"}).text.strip()\n",
    "        overall_reviews = item.find(\"div\", {\"class\": \"keyword\"}).text.strip()\n",
    "        privates_from_price = item.find(\"div\", {\"class\": \"price-col\"})\n",
    "        privates_from_price = privates_from_price.find(\"a\").text.strip() if privates_from_price else \"-\"\n",
    "        dorms_from_price = item.find(\"div\", {\"class\": \"price-col-alt\"})\n",
    "        dorms_from_price = dorms_from_price.find(\"a\").text.strip() if dorms_from_price else \"-\"\n",
    "        facilities = item.find(\"div\", {\"class\": \"facilities-label\"}).text.strip()\n",
    "        description = item.find(\"div\", {\"class\": \"additional-info\"}).text.strip()\n",
    "\n",
    "        data.append({\n",
    "            \"Hostel Name\": hostel_name,\n",
    "            \"Distance\": distance,\n",
    "            \"Rating\": rating,\n",
    "            \"Total Reviews\": total_reviews,\n",
    "            \"Overall Reviews\": overall_reviews,\n",
    "            \"Privates from Price\": privates_from_price,\n",
    "            \"Dorms from Price\": dorms_from_price,\n",
    "            \"Facilities\": facilities,\n",
    "            \"Description\": description\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape the hostel data\n",
    "hostels = scrape_hostels()\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(hostels)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"hostels_data.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
