{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# set the URL for the webpage to be scraped\n",
    "url = 'https://www.naukri.com/'\n",
    "\n",
    "# send GET request to the URL and store response in a variable\n",
    "response = requests.get(url)\n",
    "\n",
    "# create a BeautifulSoup object by parsing the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the search fields for job title and location\n",
    "job_title = soup.find('input', {'placeholder': 'Skills, Designations, Companies'})\n",
    "location = soup.find('input', {'placeholder': 'Enter the location'})\n",
    "\n",
    "# set the search values for job title and location\n",
    "job_title_value = 'Data Analyst'\n",
    "location_value = 'Bangalore'\n",
    "\n",
    "# set the values of job title and location fields\n",
    "job_title['value'] = job_title_value\n",
    "location['value'] = location_value\n",
    "\n",
    "# find the search button and click it\n",
    "search_button = soup.find('button', {'class': 'btn'})\n",
    "response = requests.post(url, data={'field-keywords': job_title_value, 'location': location_value})\n",
    "\n",
    "# create a new BeautifulSoup object for the search results page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find all the job listings on the page\n",
    "job_listings = soup.find_all('article', {'class': 'jobTuple bgWhite br4 mb-8'})\n",
    "\n",
    "# create empty lists to store the job data\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# iterate through the job listings and extract the relevant data\n",
    "for job in job_listings[:10]:\n",
    "    job_title = job.find('a', {'class': 'title fw500 ellipsis'})\n",
    "    job_location = job.find('li', {'class': 'fleft grey-text br2 placeHolderLi location'})\n",
    "    company_name = job.find('a', {'class': 'subTitle ellipsis fleft'})\n",
    "    experience = job.find('li', {'class': 'fleft grey-text br2 placeHolderLi experience'})\n",
    "    if job_title is not None:\n",
    "        job_titles.append(job_title.text.strip())\n",
    "    if job_location is not None:\n",
    "        job_locations.append(job_location.text.strip())\n",
    "    if company_name is not None:\n",
    "        company_names.append(company_name.text.strip())\n",
    "    if experience is not None:\n",
    "        experience_required.append(experience.text.strip())\n",
    "\n",
    "# create a Pandas dataframe with the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Job Title': job_titles,\n",
    "    'Job Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "})\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dde002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# set the URL for the webpage to be scraped\n",
    "url = 'https://www.naukri.com/'\n",
    "\n",
    "# send GET request to the URL and store response in a variable\n",
    "response = requests.get(url)\n",
    "\n",
    "# create a BeautifulSoup object by parsing the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the search fields for job title and location\n",
    "job_title = soup.find('input', {'placeholder': 'Skills, Designations, Companies'})\n",
    "location = soup.find('input', {'placeholder': 'Enter the location'})\n",
    "\n",
    "# set the search values for job title and location\n",
    "job_title_value = 'Data Scientist'\n",
    "location_value = 'Bangalore'\n",
    "\n",
    "# set the values of job title and location fields\n",
    "job_title['value'] = job_title_value\n",
    "location['value'] = location_value\n",
    "\n",
    "# find the search button and click it\n",
    "search_button = soup.find('button', {'class': 'btn'})\n",
    "response = requests.post(url, data={'field-keywords': job_title_value, 'location': location_value})\n",
    "\n",
    "# create a new BeautifulSoup object for the search results page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find all the job listings on the page\n",
    "job_listings = soup.find_all('article', {'class': 'jobTuple bgWhite br4 mb-8'})\n",
    "\n",
    "# create empty lists to store the job data\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "\n",
    "# iterate through the job listings and extract the relevant data\n",
    "for job in job_listings[:10]:\n",
    "    job_title = job.find('a', {'class': 'title fw500 ellipsis'})\n",
    "    job_location = job.find('li', {'class': 'fleft grey-text br2 placeHolderLi location'})\n",
    "    company_name = job.find('a', {'class': 'subTitle ellipsis fleft'})\n",
    "    if job_title is not None:\n",
    "        job_titles.append(job_title.text.strip())\n",
    "    if job_location is not None:\n",
    "        job_locations.append(job_location.text.strip())\n",
    "    if company_name is not None:\n",
    "        company_names.append(company_name.text.strip())\n",
    "\n",
    "# create a Pandas dataframe with the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Job Title': job_titles,\n",
    "    'Job Location': job_locations,\n",
    "    'Company Name': company_names\n",
    "})\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# set the URL for the Flipkart webpage\n",
    "url = 'https://www.flipkart.com/'\n",
    "\n",
    "# set the search term\n",
    "search_term = 'sunglasses'\n",
    "\n",
    "# send GET request to the URL for the Flipkart webpage with the search term as parameter\n",
    "response = requests.get(url + f'search?q={search_term}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&sort=price_asc&page=')\n",
    "\n",
    "# create empty lists to store the sunglass data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# iterate through the search result pages to scrape data for the first 100 sunglasses\n",
    "for page in range(1, 11):\n",
    "    # create a BeautifulSoup object by parsing the HTML content of the current search result page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # find all the sunglass listings on the page\n",
    "    listings = soup.find_all('div', {'class': '_2kHMtA'})\n",
    "\n",
    "    # iterate through the sunglass listings and extract the relevant data\n",
    "    for listing in listings:\n",
    "        # find the brand of the sunglass\n",
    "        brand = listing.find('div', {'class': '_2WkVRV'})\n",
    "        if brand is not None:\n",
    "            brands.append(brand.text)\n",
    "\n",
    "        # find the product description of the sunglass\n",
    "        description = listing.find('a', {'class': '_2mylT6'})\n",
    "        if description is not None:\n",
    "            descriptions.append(description.text)\n",
    "\n",
    "        # find the price of the sunglass\n",
    "        price = listing.find('div', {'class': '_30jeq3 _1_WHN1'})\n",
    "        if price is not None:\n",
    "            prices.append(price.text)\n",
    "\n",
    "        # break the loop if we have scraped data for 100 sunglasses\n",
    "        if len(brands) == 100:\n",
    "            break\n",
    "\n",
    "    # break the loop if we have scraped data for 100 sunglasses\n",
    "    if len(brands) == 100:\n",
    "        break\n",
    "\n",
    "    # get the link to the next search result page\n",
    "    next_page_link = soup.find('a', {'class': '_1LKTO3'})['href']\n",
    "\n",
    "    # send GET request to the URL for the next search result page and store response in a variable\n",
    "    response = requests.get(url + next_page_link)\n",
    "\n",
    "# create a Pandas dataframe with the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Brand': brands,\n",
    "    'Product Description': descriptions,\n",
    "    'Price': prices\n",
    "})\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4962193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set the URL for iPhone 11 reviews on Flipkart\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Create empty lists to store the scraped data\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "# Loop through the first 10 pages of reviews (each page contains 10 reviews)\n",
    "for i in range(10):\n",
    "    # Get the page content\n",
    "    page_url = url + \"&page=\" + str(i)\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all the review blocks on the page\n",
    "    review_blocks = soup.find_all('div', {'class': '_3gijNv col-12-12'})\n",
    "    for block in review_blocks:\n",
    "        # Scrape the rating\n",
    "        rating = block.find('div', {'class': '_3LWZlK _1BLPMq'}).text\n",
    "        ratings.append(rating)\n",
    "        \n",
    "        # Scrape the review summary\n",
    "        review_summary = block.find('p', {'class': '_2-N8zT'}).text\n",
    "        review_summaries.append(review_summary)\n",
    "        \n",
    "        # Scrape the full review\n",
    "        full_review = block.find('div', {'class': 't-ZTKy'}).div.div.text\n",
    "        full_reviews.append(full_review)\n",
    "        \n",
    "        # Stop scraping if we have collected 100 reviews\n",
    "        if len(ratings) >= 100:\n",
    "            break\n",
    "            \n",
    "# Create a pandas dataframe with the scraped data\n",
    "df = pd.DataFrame({'Rating': ratings, 'Review Summary': review_summaries, 'Full Review': full_reviews})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# initialize lists for storing scraped data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# set up the URL and headers for the request\n",
    "url = \"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "\n",
    "# loop through the first 10 pages of search results\n",
    "for i in range(1, 11):\n",
    "    page_url = url + \"&page=\" + str(i)\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    results = soup.find_all('div', attrs={'class': '_2kHMtA'})\n",
    "    \n",
    "    # loop through each sneaker on the page and extract the brand, description, and price\n",
    "    for sneaker in results:\n",
    "        brand = sneaker.find('div', attrs={'class': '_2WkVRV'}).text\n",
    "        description = sneaker.find('a', attrs={'class': 'IRpwTa'}).text\n",
    "        price = sneaker.find('div', attrs={'class': '_30jeq3 _1_WHN1'}).text.replace('₹', '')\n",
    "        \n",
    "        brands.append(brand)\n",
    "        descriptions.append(description)\n",
    "        prices.append(price)\n",
    "\n",
    "# create a Pandas dataframe from the scraped data\n",
    "df = pd.DataFrame({'Brand': brands, 'Description': descriptions, 'Price': prices})\n",
    "\n",
    "# output the dataframe to a CSV file\n",
    "df.to_csv('sneakers.csv', index=False)\n",
    "\n",
    "print('Scraping complete. Data saved to \"sneakers.csv\"')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
